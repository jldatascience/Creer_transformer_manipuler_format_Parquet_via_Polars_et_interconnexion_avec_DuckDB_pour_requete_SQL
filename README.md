# ğŸ“ˆ GeÌneÌrer, transformer et manipuler un fichier Parquet (Big Data) via Polars & interconnexion avec DuckDB pour exeÌcuter des requeÌ‚tes SQL


## DÃ©marche

Dans ce notebook ci-joint, nous allons mettre en pratique lâ€™ensemble des points abordÃ©s ci-dessous.




## Polars : un package Python plus performant que Pandas et permettant de manipuler les donnÃ©es tabulaires Ã  partir de diffÃ©rents types de fichiers tels que Parquet, CSV

Polars est un package Python permettant de manipuler les donnÃ©es tabulaires Ã  partir de diffÃ©rents types de fichiers (CSV, Parquet, etc.).
Il est une alternative directe et moderne Ã  Pandas, pensÃ©e pour Ãªtre trÃ¨s performante tout en offrant une syntaxe comprÃ©hensible Ã  pour des data scientists habituÃ©s Ã  dâ€™autres frameworks de manipulation de donnÃ©es comme dplyr.




## Les fichiers Parquet ont un format optimisÃ© pour stocker et interroger de faÃ§on efficace du Big Data

Les fichiers Parquet ont un format binaire beaucoup plus lÃ©ger que les fichiers CSV pour le stockage des tableaux de donnÃ©es.
Ex : faire une requÃªte SQL quasi instantanÃ©e sur des millions de lignes rÃ©partis sur diffÃ©rents fichiers parquets
En effet, aujourd'hui le format parquet est utilisÃ© en big data pour stocker et interroger de faÃ§on efficace des donnÃ©es volumineuses grÃ¢ce Ã  un modÃ¨le orientÃ© colonne basÃ©e sur Apache Arrow. Ã€ cela, s'ajoute de nouveaux outils en Python pour pouvoir manipuler et interroger ces fichiers.



## Architecture d'un fichier parquet : Base de donnÃ©es orientÃ©e colonnes

Dans un fichier parquet, ce sont les colonnes qui sont sauvegardÃ©es de maniÃ¨re contiguÃ« (collÃ©) en mÃ©moire.
Ceci permet de faire des opÃ©rations de faÃ§on trÃ¨s efficace sur les colonnes au dÃ©triment des opÃ©rations transactionnelles. Cette architecture est trÃ¨s performante pour des systÃ¨mes OLAP (OnLine Analytical Processing). Par exemple un entrepÃ´t de donnÃ©es destinÃ©s Ã  Ãªtre lu uniquement.




## Le format parquet, dÃ©veloppÃ© par Apache, est entiÃ¨rement compatible avec Arrow

Apache Arrow est un format standard de donnÃ©e orientÃ© colonne pour la mÃ©moire vive. C'est-Ã -dire qu'il dÃ©crit, indÃ©pendamment du langage de programmation,comment reprÃ©senter un tableau dans votre RAM.
Par exemple, si vous manipulez les mÃªmes donnÃ©es stockÃ©es dans un DataFrame Python ou un DataFrame R, la structure mÃ©moire sous-jacent sera la mÃªme. Autrement dit, vous allez pouvoir transfÃ©rer un DataFrame Python vers un DataFrame R, sans faire la moindre copie ou transformation. Et lorsque l'on travaille avec beaucoup de donnÃ©es, cela est loin d'Ãªtre nÃ©gligeable.

Le format parquet, dÃ©veloppÃ© par Apache, est entiÃ¨rement compatible avec Arrow. La sÃ©rialisation et la dÃ©serialisation d'un DataFrame, c'est Ã  dire l'Ã©criture et la lecture d'un fichier parquet sera trÃ¨s performante avec un minimum de transformation.

Sans Arrow, il est nÃ©cessaire de faire des conversions et des copies coÃ»teuses entre les diffÃ©rentes sources de donnÃ©es. Le format mÃ©moire agnostique d'Apache Arrow permet d'Ã©viter toutes ces opÃ©rations coÃ»teuses

Sans Arrow, il est nÃ©cessaire de faire des conversions et des copies coÃ»teuses entre les diffÃ©rentes sources de donnÃ©es. Le format mÃ©moire agnostique d'Apache Arrow permet d'Ã©viter toutes ces opÃ©rations coÃ»teuses




## Une interconnexion avec DuckDB pour faire des requÃªtes SQL

Polars sâ€™appuie sur Apache Arrow, un Ã©cosystÃ¨me pour lire des donnÃ©es (CSV ou Parquet) de maniÃ¨re trÃ¨s efficace et les stocker dans des objets peu volumineux pour gagner en performance. Câ€™est Ã©galement le cas dâ€™une autre librairie intÃ©ressante pour le traitement de donnÃ©es: DuckDB.

PlutÃ´t que concurrentes, ces deux librairies sont trÃ¨s complÃ©mentaires. Il est possible dâ€™effectuer des requÃªtes SQL depuis un DataFrame Polars via DuckDB ou dâ€™utiliser DuckDB pour lire les donnÃ©es et convertir celles-ci en DataFrame Polars Ã  lâ€™issue des requÃªtes. Ces deux approches sont intÃ©ressantes pour les personnes familiÃ¨res avec le langage SQL.

